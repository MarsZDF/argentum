{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "context_decay_header"
   },
   "source": [
    "# üß† ContextDecay: Advanced Memory Management & Cost Optimization\n",
    "\n",
    "Welcome to the complete guide to ContextDecay! This notebook teaches you how to build smart memory management systems that automatically optimize agent performance and costs.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "Master the art of agent memory management:\n",
    "- **Temporal decay models**: How memories naturally fade over time\n",
    "- **Importance-based retention**: Keep critical information longer\n",
    "- **Cost optimization**: Balance memory richness with budget constraints\n",
    "- **Performance tuning**: Scale memory systems for production workloads\n",
    "- **Real-world applications**: Conversational AI, multi-session agents, and more\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_context_decay"
   },
   "outputs": [],
   "source": [
    "# Install Argentum if needed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import argentum\n",
    "    print(\"‚úÖ Argentum already installed\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing Argentum...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"argentum-agent\"])\n",
    "    print(\"‚úÖ Installation complete!\")\n",
    "\n",
    "# Import required modules\n",
    "from argentum import ContextDecay\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"üß† ContextDecay Tutorial - Argentum v{argentum.__version__}\")\n",
    "print(f\"üìÖ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Chapter 1: Understanding Temporal Decay\n",
    "\n",
    "Learn the mathematical foundations and practical implications of memory decay in AI agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö Chapter 1: Understanding Temporal Decay\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Mathematical foundation\n",
    "print(\"üßÆ Mathematical Foundation:\")\n",
    "print(\"The default decay function follows exponential decay:\")\n",
    "print(\"   current_weight = importance √ó (0.5 ^ (steps_elapsed / half_life))\")\n",
    "print(\"\")\n",
    "print(\"This ensures that after 'half_life' steps, an item retains 50% of its original importance.\")\n",
    "\n",
    "# Demonstrate basic decay with visualization\n",
    "print(\"\\nüìä Decay Visualization:\")\n",
    "\n",
    "def exponential_decay(importance, steps, half_life):\n",
    "    \"\"\"Standard exponential decay function\"\"\"\n",
    "    return importance * (0.5 ** (steps / half_life))\n",
    "\n",
    "# Different half-life scenarios\n",
    "half_lives = [5, 10, 20]\n",
    "importance = 1.0\n",
    "max_steps = 30\n",
    "\n",
    "print(f\"Tracking decay over {max_steps} steps for different half-lives:\")\n",
    "print(\"Steps | Half-life=5 | Half-life=10 | Half-life=20\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for step in range(0, max_steps + 1, 5):\n",
    "    weights = [exponential_decay(importance, step, hl) for hl in half_lives]\n",
    "    print(f\"{step:5d} | {weights[0]:11.3f} | {weights[1]:12.3f} | {weights[2]:12.3f}\")\n",
    "\n",
    "# Practical example: Conversational AI memory\n",
    "print(\"\\nüí¨ Practical Example: Conversational AI Memory\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Initialize conversation agent with memory\n",
    "conversation_memory = ContextDecay(half_life_steps=8)\n",
    "\n",
    "print(\"ü§ñ Simulating a conversation about vacation planning:\")\n",
    "\n",
    "# Conversation simulation\n",
    "conversation_turns = [\n",
    "    (\"user_name\", \"Sarah\", 0.9, \"Hi, I'm Sarah and I need help planning a vacation\"),\n",
    "    (\"destination_interest\", \"Europe\", 0.8, \"I'm thinking about Europe, maybe Italy or France\"),\n",
    "    (\"budget_range\", \"3000_5000\", 0.9, \"My budget is between $3000-5000\"),\n",
    "    (\"travel_dates\", \"June_2024\", 0.9, \"I want to travel in June 2024\"),\n",
    "    (\"group_size\", \"couple\", 0.7, \"It's just me and my partner\"),\n",
    "    (\"accommodation_pref\", \"boutique_hotels\", 0.6, \"We prefer boutique hotels over chains\"),\n",
    "    (\"activity_interest\", \"culture_food\", 0.8, \"We're interested in culture and food experiences\"),\n",
    "    (\"specific_destination\", \"Tuscany\", 1.0, \"Actually, let's focus on Tuscany specifically\"),\n",
    "    (\"duration\", \"10_days\", 0.8, \"We're thinking about 10 days\"),\n",
    "    (\"transport_pref\", \"rental_car\", 0.7, \"We'd like to rent a car to explore\")\n",
    "]\n",
    "\n",
    "for turn, (key, value, importance, description) in enumerate(conversation_turns):\n",
    "    conversation_memory.add(key, value, importance=importance)\n",
    "    print(f\"Turn {turn + 1:2d}: {description} (importance: {importance})\")\n",
    "    \n",
    "    # Advance time to simulate conversation progression\n",
    "    conversation_memory.step()\n",
    "\n",
    "print(f\"\\nüß† Memory State After {len(conversation_turns)} Turns:\")\n",
    "stats = conversation_memory.get_stats()\n",
    "print(f\"   Total memories: {stats['total_items']}\")\n",
    "print(f\"   Active memories: {stats['active_items']} (threshold: 0.3)\")\n",
    "print(f\"   Average decay: {stats['avg_decay']:.3f}\")\n",
    "print(f\"   Oldest memory age: {stats['oldest_age']} turns\")\n",
    "\n",
    "# Show what's still remembered\n",
    "active_memories = conversation_memory.get_active(threshold=0.3)\n",
    "print(\"\\nüí≠ Still Active in Memory:\")\n",
    "for key, value, weight in active_memories:\n",
    "    original_importance = next(imp for k, v, imp, desc in conversation_turns if k == key)\n",
    "    print(f\"   {key:20s}: '{value}' (weight: {weight:.3f}, was: {original_importance})\")\n",
    "\n",
    "# Demonstrate the effect of importance on retention\n",
    "print(\"\\nüéØ Key Insight: High-importance items persist longer\")\n",
    "high_importance_items = [item for item in active_memories if item[2] > 0.5]\n",
    "print(f\"   Items with weight > 0.5: {len(high_importance_items)}/{len(active_memories)}\")\n",
    "\n",
    "critical_info = [\"specific_destination\", \"budget_range\", \"travel_dates\"]\n",
    "retained_critical = [key for key, _, _ in active_memories if key in critical_info]\n",
    "print(f\"   Critical info retained: {len(retained_critical)}/{len(critical_info)}\")\n",
    "print(f\"   Retained: {', '.join(retained_critical)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Chapter 2: Custom Decay Functions\n",
    "\n",
    "Learn to implement custom decay functions for specialized use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Chapter 2: Custom Decay Functions\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"üé® Custom Decay Functions for Different Use Cases:\")\n",
    "\n",
    "# Define various decay functions\n",
    "def linear_decay(importance, steps, half_life):\n",
    "    \"\"\"Linear decay - steady decline\"\"\"\n",
    "    decline_rate = importance / (half_life * 2)  # Reach ~0 at 2x half_life\n",
    "    return max(0, importance - (steps * decline_rate))\n",
    "\n",
    "def logarithmic_decay(importance, steps, half_life):\n",
    "    \"\"\"Logarithmic decay - fast initial decline, then slow\"\"\"\n",
    "    if steps == 0:\n",
    "        return importance\n",
    "    decay_factor = math.log(steps + 1) / math.log(half_life + 1)\n",
    "    return importance * max(0, 1 - decay_factor)\n",
    "\n",
    "def sigmoid_decay(importance, steps, half_life):\n",
    "    \"\"\"Sigmoid decay - sudden drop around half_life\"\"\"\n",
    "    steepness = 0.5\n",
    "    sigmoid_value = 1 / (1 + math.exp(steepness * (steps - half_life)))\n",
    "    return importance * sigmoid_value\n",
    "\n",
    "def stepped_decay(importance, steps, half_life):\n",
    "    \"\"\"Stepped decay - discrete reductions at intervals\"\"\"\n",
    "    step_interval = half_life / 2\n",
    "    reduction_per_step = 0.25\n",
    "    num_steps = int(steps / step_interval)\n",
    "    return importance * max(0, 1 - (num_steps * reduction_per_step))\n",
    "\n",
    "# Demonstrate different decay functions\n",
    "print(\"\\nüìä Comparing Decay Functions:\")\n",
    "decay_functions = {\n",
    "    \"Exponential\": exponential_decay,\n",
    "    \"Linear\": linear_decay,\n",
    "    \"Logarithmic\": logarithmic_decay,\n",
    "    \"Sigmoid\": sigmoid_decay,\n",
    "    \"Stepped\": stepped_decay\n",
    "}\n",
    "\n",
    "half_life = 10\n",
    "importance = 1.0\n",
    "steps_to_analyze = range(0, 21, 2)\n",
    "\n",
    "print(\"Steps | Expo | Linear | Log | Sigmoid | Stepped\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for step in steps_to_analyze:\n",
    "    weights = [decay_functions[name](importance, step, half_life) for name in decay_functions.keys()]\n",
    "    print(f\"{step:5d} | {weights[0]:4.2f} | {weights[1]:6.2f} | {weights[2]:3.2f} | {weights[3]:7.2f} | {weights[4]:7.2f}\")\n",
    "\n",
    "# Use case scenarios for each decay function\n",
    "print(\"\\nüéØ Use Case Scenarios:\")\n",
    "use_cases = {\n",
    "    \"Exponential\": \"General purpose - natural forgetting pattern\",\n",
    "    \"Linear\": \"Scheduled tasks - predictable timeline decay\",\n",
    "    \"Logarithmic\": \"Breaking news - important initially, then stable\",\n",
    "    \"Sigmoid\": \"Deadline-based - sudden relevance drop after date\",\n",
    "    \"Stepped\": \"Versioned data - discrete update cycles\"\n",
    "}\n",
    "\n",
    "for decay_type, use_case in use_cases.items():\n",
    "    print(f\"   {decay_type:12s}: {use_case}\")\n",
    "\n",
    "# Practical example: News aggregation agent\n",
    "print(\"\\nüì∞ Practical Example: News Aggregation Agent\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Different news types need different decay patterns\n",
    "news_contexts = {\n",
    "    \"breaking_news\": ContextDecay(half_life_steps=6, decay_function=logarithmic_decay),\n",
    "    \"market_updates\": ContextDecay(half_life_steps=4, decay_function=exponential_decay),\n",
    "    \"feature_stories\": ContextDecay(half_life_steps=12, decay_function=linear_decay),\n",
    "    \"scheduled_events\": ContextDecay(half_life_steps=8, decay_function=sigmoid_decay)\n",
    "}\n",
    "\n",
    "# Simulate news items\n",
    "news_items = [\n",
    "    (\"breaking_news\", \"major_earthquake\", \"7.2 earthquake hits Japan\", 1.0),\n",
    "    (\"market_updates\", \"tech_stock_rise\", \"Tech stocks up 3% on AI news\", 0.8),\n",
    "    (\"feature_stories\", \"climate_report\", \"Annual climate change report released\", 0.7),\n",
    "    (\"scheduled_events\", \"election_results\", \"Election results expected tonight\", 0.9),\n",
    "    (\"breaking_news\", \"celebrity_news\", \"Celebrity announces retirement\", 0.6),\n",
    "    (\"market_updates\", \"oil_prices\", \"Oil prices drop 2% on supply news\", 0.7)\n",
    "]\n",
    "\n",
    "print(\"Adding news items to specialized memory contexts:\")\n",
    "for category, key, description, importance in news_items:\n",
    "    news_contexts[category].add(key, description, importance=importance)\n",
    "    print(f\"   {category:16s}: {description} (importance: {importance})\")\n",
    "\n",
    "# Simulate time passing (12 time steps)\n",
    "print(\"\\n‚è∞ Simulating 12 time steps...\")\n",
    "for step in range(12):\n",
    "    for context in news_contexts.values():\n",
    "        context.step()\n",
    "\n",
    "# Analyze what's still relevant\n",
    "print(\"\\nüìä News Relevance After 12 Time Steps:\")\n",
    "threshold = 0.2\n",
    "\n",
    "for category, context in news_contexts.items():\n",
    "    active_news = context.get_active(threshold=threshold)\n",
    "    print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "    \n",
    "    if active_news:\n",
    "        for key, description, weight in active_news:\n",
    "            original_importance = next(imp for cat, k, desc, imp in news_items if k == key)\n",
    "            print(f\"   {description[:40]:40s} (weight: {weight:.3f}, was: {original_importance})\")\n",
    "    else:\n",
    "        print(\"   No items above relevance threshold\")\n",
    "\n",
    "# Insights about decay patterns\n",
    "print(\"\\nüí° Decay Pattern Insights:\")\n",
    "print(\"   üìà Logarithmic (breaking news): High initial retention, gradual decline\")\n",
    "print(\"   üìâ Exponential (market updates): Steady natural decay\")\n",
    "print(\"   üìä Linear (feature stories): Predictable steady decline\")\n",
    "print(\"   üìã Sigmoid (scheduled events): Sharp drop after relevance period\")\n",
    "\n",
    "# Best practices for custom decay functions\n",
    "print(\"\\nüéØ Best Practices for Custom Decay Functions:\")\n",
    "best_practices = [\n",
    "    \"Match decay pattern to information lifecycle\",\n",
    "    \"Consider domain-specific relevance patterns\",\n",
    "    \"Test decay functions with realistic time scales\",\n",
    "    \"Balance retention with memory efficiency\",\n",
    "    \"Document decay rationale for team understanding\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"   ‚Ä¢ {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Chapter 3: Cost-Optimized Memory Management\n",
    "\n",
    "Learn to balance memory richness with budget constraints using Argentum's cost optimization features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí∞ Chapter 3: Cost-Optimized Memory Management\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üéØ Scenario: Cost-conscious customer service agent\")\n",
    "print(\"Challenge: Maintain service quality while controlling memory costs\")\n",
    "\n",
    "# Initialize cost-optimized context manager\n",
    "cost_optimized_memory = ContextDecay(\n",
    "    half_life_steps=10,\n",
    "    cost_optimization=True,\n",
    "    max_context_cost=0.50  # 50 cents maximum\n",
    ")\n",
    "\n",
    "print(f\"\\nüí∏ Initialized with ${cost_optimized_memory._max_context_cost:.2f} budget\")\n",
    "\n",
    "# Simulate customer service interactions with varying storage costs\n",
    "print(\"\\nü§ù Customer Service Interaction Simulation:\")\n",
    "\n",
    "customer_interactions = [\n",
    "    # (key, value, importance, storage_cost, description)\n",
    "    (\"customer_name\", \"Jennifer Martinez\", 0.9, 0.02, \"Customer identification\"),\n",
    "    (\"issue_type\", \"billing_discrepancy\", 1.0, 0.03, \"Primary issue to resolve\"),\n",
    "    (\"account_history\", \"premium_member_3_years\", 0.8, 0.05, \"Account context\"),\n",
    "    (\"previous_issues\", [\"late_payment_2023\", \"upgrade_request_2024\"], 0.6, 0.08, \"Historical context\"),\n",
    "    (\"current_bill\", {\"amount\": 89.99, \"due_date\": \"2024-11-15\", \"disputed_charges\": [\"tax_error\"]}, 0.95, 0.12, \"Current billing details\"),\n",
    "    (\"customer_sentiment\", \"frustrated_but_polite\", 0.7, 0.04, \"Interaction tone\"),\n",
    "    (\"resolution_attempts\", [], 0.8, 0.06, \"Troubleshooting history\"),\n",
    "    (\"agent_notes\", \"customer_very_reasonable_easy_to_work_with\", 0.5, 0.07, \"Agent observations\"),\n",
    "    (\"escalation_path\", \"supervisor_if_no_resolution\", 0.6, 0.05, \"Next steps if needed\"),\n",
    "    (\"resolution_status\", \"investigating\", 0.9, 0.04, \"Current status\"),\n",
    "    (\"callback_preference\", \"email_preferred_phone_backup\", 0.4, 0.06, \"Communication preferences\"),\n",
    "    (\"system_logs\", [\"login_09:30\", \"bill_viewed_09:31\", \"dispute_filed_09:32\"], 0.3, 0.15, \"Technical logs\"),\n",
    "    (\"related_customers\", [\"same_household_account\", \"family_plan_member\"], 0.4, 0.10, \"Related accounts\"),\n",
    "    (\"payment_history\", \"excellent_never_late\", 0.7, 0.08, \"Payment reliability\"),\n",
    "    (\"special_offers\", \"retention_discount_available\", 0.2, 0.09, \"Available promotions\")\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(customer_interactions)} pieces of customer context:\")\n",
    "total_attempted_cost = 0\n",
    "\n",
    "for key, value, importance, storage_cost, description in customer_interactions:\n",
    "    total_attempted_cost += storage_cost\n",
    "    print(f\"   Adding: {description} (${storage_cost:.3f}, importance: {importance})\")\n",
    "    \n",
    "    # Add to memory with cost tracking\n",
    "    cost_optimized_memory.add(key, value, importance=importance, storage_cost=storage_cost)\n",
    "    \n",
    "    # Check current cost status\n",
    "    current_cost = cost_optimized_memory._current_cost\n",
    "    if current_cost > cost_optimized_memory._max_context_cost * 0.8:\n",
    "        print(f\"      ‚ö†Ô∏è  Approaching budget limit: ${current_cost:.3f}\")\n",
    "\n",
    "print(f\"\\nüí∞ Cost Management Results:\")\n",
    "print(f\"   Attempted total cost: ${total_attempted_cost:.3f}\")\n",
    "print(f\"   Actual cost: ${cost_optimized_memory._current_cost:.3f}\")\n",
    "print(f\"   Budget: ${cost_optimized_memory._max_context_cost:.3f}\")\n",
    "print(f\"   Cost savings: ${total_attempted_cost - cost_optimized_memory._current_cost:.3f}\")\n",
    "\n",
    "# Analyze what was kept vs. pruned\n",
    "active_context = cost_optimized_memory.get_active(threshold=0.1)\n",
    "print(f\"\\nüß† Memory Analysis:\")\n",
    "print(f\"   Items stored: {len(active_context)}/{len(customer_interactions)}\")\n",
    "print(f\"   Budget utilization: {(cost_optimized_memory._current_cost / cost_optimized_memory._max_context_cost) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nüìã Context Retained (sorted by cost effectiveness):\")\n",
    "cost_effectiveness_data = []\n",
    "for key, value, weight in active_context:\n",
    "    # Find original data\n",
    "    original_data = next((item for item in customer_interactions if item[0] == key), None)\n",
    "    if original_data:\n",
    "        _, _, importance, storage_cost, description = original_data\n",
    "        cost_effectiveness = importance / storage_cost if storage_cost > 0 else importance\n",
    "        cost_effectiveness_data.append((description, importance, storage_cost, cost_effectiveness, weight))\n",
    "\n",
    "# Sort by cost effectiveness\n",
    "cost_effectiveness_data.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "for description, importance, storage_cost, cost_eff, weight in cost_effectiveness_data:\n",
    "    print(f\"   {description[:35]:35s} | Imp: {importance:.1f} | Cost: ${storage_cost:.3f} | Eff: {cost_eff:.1f} | Weight: {weight:.3f}\")\n",
    "\n",
    "# Get detailed cost report\n",
    "cost_report = cost_optimized_memory.get_cost_report()\n",
    "print(f\"\\nüìä Detailed Cost Report:\")\n",
    "print(f\"   Total items: {cost_report['total_items']}\")\n",
    "print(f\"   Total cost: ${cost_report['total_cost']:.3f}\")\n",
    "print(f\"   Budget utilization: {cost_report['budget_utilization']:.1%}\")\n",
    "print(f\"   Average cost per item: ${cost_report['average_cost_per_item']:.3f}\")\n",
    "print(f\"   Cost efficiency: {cost_report['cost_efficiency']:.1f} importance/$\")\n",
    "print(f\"   Items pruned: {cost_report['items_pruned']}\")\n",
    "print(f\"   Cost saved through pruning: ${cost_report['cost_saved']:.3f}\")\n",
    "\n",
    "# Demonstrate dynamic cost adjustment\n",
    "print(\"\\n‚öñÔ∏è Dynamic Cost Adjustment Demo:\")\n",
    "print(\"Simulating memory pressure with high-cost items...\")\n",
    "\n",
    "# Try to add expensive items\n",
    "expensive_items = [\n",
    "    (\"full_call_transcript\", \"[Very long transcript with 1000+ words...]\", 0.8, 0.25),\n",
    "    (\"detailed_product_catalog\", \"[Complete product database cache]\", 0.3, 0.20),\n",
    "    (\"ai_model_cache\", \"[Cached model weights and embeddings]\", 0.4, 0.30)\n",
    "]\n",
    "\n",
    "for key, value, importance, storage_cost in expensive_items:\n",
    "    initial_cost = cost_optimized_memory._current_cost\n",
    "    cost_optimized_memory.add(key, value, importance=importance, storage_cost=storage_cost)\n",
    "    final_cost = cost_optimized_memory._current_cost\n",
    "    \n",
    "    cost_change = final_cost - initial_cost\n",
    "    print(f\"   {key}: Attempted ${storage_cost:.3f}, actual cost change: ${cost_change:.3f}\")\n",
    "\n",
    "# Final memory state\n",
    "final_active = cost_optimized_memory.get_active(threshold=0.1)\n",
    "final_cost_report = cost_optimized_memory.get_cost_report()\n",
    "\n",
    "print(f\"\\nüéØ Final Memory State:\")\n",
    "print(f\"   Active items: {len(final_active)}\")\n",
    "print(f\"   Total cost: ${final_cost_report['total_cost']:.3f}\")\n",
    "print(f\"   Budget compliance: {'‚úÖ' if final_cost_report['budget_utilization'] <= 1.0 else '‚ùå'}\")\n",
    "print(f\"   Cost efficiency: {final_cost_report['cost_efficiency']:.1f} importance/$\")\n",
    "\n",
    "# Cost optimization strategies\n",
    "print(\"\\nüí° Cost Optimization Strategies Demonstrated:\")\n",
    "strategies = [\n",
    "    \"Automatic pruning of low cost-effectiveness items\",\n",
    "    \"Importance-based budget allocation\",\n",
    "    \"Dynamic cost adjustment for high-value items\",\n",
    "    \"Real-time budget monitoring and compliance\",\n",
    "    \"Cost-effectiveness ranking for retention decisions\"\n",
    "]\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"   ‚úÖ {strategy}\")\n",
    "\n",
    "# Production recommendations\n",
    "print(\"\\nüöÄ Production Cost Management Recommendations:\")\n",
    "recommendations = [\n",
    "    \"Set budget limits based on business value of memory retention\",\n",
    "    \"Monitor cost efficiency metrics in real-time\",\n",
    "    \"Implement tiered storage for different importance levels\",\n",
    "    \"Use cost forecasting to predict memory budget needs\",\n",
    "    \"Regular cost audits to optimize storage strategies\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"   üìã {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé™ Chapter 4: Multi-Agent Memory Coordination\n",
    "\n",
    "Learn to manage shared memory contexts across multiple agents with different decay requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé™ Chapter 4: Multi-Agent Memory Coordination\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üéØ Scenario: Multi-agent customer support system\")\n",
    "print(\"Agents: Intake ‚Üí Specialist ‚Üí Follow-up\")\n",
    "\n",
    "# Define different agent types with specialized memory needs\n",
    "class MultiAgentMemoryCoordinator:\n",
    "    def __init__(self):\n",
    "        # Different agents have different memory priorities\n",
    "        self.agents = {\n",
    "            \"intake_agent\": ContextDecay(\n",
    "                half_life_steps=5,  # Fast turnover, handles many customers\n",
    "                cost_optimization=True,\n",
    "                max_context_cost=0.25\n",
    "            ),\n",
    "            \"specialist_agent\": ContextDecay(\n",
    "                half_life_steps=15,  # Longer memory for complex problem-solving\n",
    "                cost_optimization=True,\n",
    "                max_context_cost=0.75\n",
    "            ),\n",
    "            \"followup_agent\": ContextDecay(\n",
    "                half_life_steps=20,  # Longest memory for relationship building\n",
    "                cost_optimization=True,\n",
    "                max_context_cost=0.50\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Shared context that all agents can access\n",
    "        self.shared_context = ContextDecay(\n",
    "            half_life_steps=25,  # Long-term shared memory\n",
    "            cost_optimization=True,\n",
    "            max_context_cost=1.00\n",
    "        )\n",
    "        \n",
    "    def transfer_context(self, from_agent: str, to_agent: str, context_keys: list):\n",
    "        \"\"\"Transfer context between agents with importance adjustment\"\"\"\n",
    "        source = self.agents[from_agent]\n",
    "        target = self.agents[to_agent]\n",
    "        \n",
    "        transferred = []\n",
    "        for key in context_keys:\n",
    "            if key in source._items:\n",
    "                item = source._items[key]\n",
    "                current_weight = source._calculate_current_weight(item)\n",
    "                \n",
    "                # Adjust importance based on target agent's needs\n",
    "                importance_multiplier = self._get_importance_multiplier(from_agent, to_agent, key)\n",
    "                adjusted_importance = min(1.0, item['importance'] * importance_multiplier)\n",
    "                \n",
    "                # Transfer to target agent\n",
    "                target.add(\n",
    "                    key, \n",
    "                    item['value'], \n",
    "                    importance=adjusted_importance,\n",
    "                    storage_cost=item.get('storage_cost', 0.02)\n",
    "                )\n",
    "                \n",
    "                transferred.append((key, current_weight, adjusted_importance))\n",
    "        \n",
    "        return transferred\n",
    "    \n",
    "    def _get_importance_multiplier(self, from_agent: str, to_agent: str, key: str) -> float:\n",
    "        \"\"\"Adjust importance based on agent transition\"\"\"\n",
    "        # Define what each agent type values most\n",
    "        agent_priorities = {\n",
    "            \"intake_agent\": [\"customer_info\", \"issue_type\", \"urgency\"],\n",
    "            \"specialist_agent\": [\"technical_details\", \"troubleshooting_history\", \"solution_attempts\"],\n",
    "            \"followup_agent\": [\"resolution_status\", \"customer_satisfaction\", \"follow_up_schedule\"]\n",
    "        }\n",
    "        \n",
    "        # Boost importance if the key is high priority for target agent\n",
    "        if any(priority in key for priority in agent_priorities.get(to_agent, [])):\n",
    "            return 1.2  # 20% boost\n",
    "        \n",
    "        # Reduce importance if it was specific to source agent\n",
    "        if any(priority in key for priority in agent_priorities.get(from_agent, [])):\n",
    "            return 0.8  # 20% reduction\n",
    "        \n",
    "        return 1.0  # No change\n",
    "    \n",
    "    def add_to_shared(self, key: str, value, importance: float, storage_cost: float = 0.05):\n",
    "        \"\"\"Add important context to shared memory\"\"\"\n",
    "        self.shared_context.add(key, value, importance=importance, storage_cost=storage_cost)\n",
    "    \n",
    "    def get_agent_stats(self):\n",
    "        \"\"\"Get statistics for all agents\"\"\"\n",
    "        stats = {}\n",
    "        for agent_name, agent in self.agents.items():\n",
    "            stats[agent_name] = {\n",
    "                \"memory_stats\": agent.get_stats(),\n",
    "                \"cost_report\": agent.get_cost_report(),\n",
    "                \"active_items\": len(agent.get_active(threshold=0.2))\n",
    "            }\n",
    "        \n",
    "        stats[\"shared\"] = {\n",
    "            \"memory_stats\": self.shared_context.get_stats(),\n",
    "            \"cost_report\": self.shared_context.get_cost_report(),\n",
    "            \"active_items\": len(self.shared_context.get_active(threshold=0.2))\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Initialize the coordinator\n",
    "coordinator = MultiAgentMemoryCoordinator()\n",
    "print(\"‚úÖ Multi-agent memory coordinator initialized\")\n",
    "\n",
    "# Simulate customer support workflow\n",
    "print(\"\\nüé¨ Customer Support Workflow Simulation:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Phase 1: Intake Agent\n",
    "print(\"\\n1Ô∏è‚É£ Intake Agent Phase:\")\n",
    "intake_data = [\n",
    "    (\"customer_info\", \"David Chen, Account #12789\", 0.9, 0.03),\n",
    "    (\"issue_type\", \"internet_connectivity_problem\", 1.0, 0.02),\n",
    "    (\"urgency\", \"high_business_critical\", 0.95, 0.02),\n",
    "    (\"customer_tier\", \"enterprise_platinum\", 0.8, 0.04),\n",
    "    (\"initial_symptoms\", \"intermittent_disconnections_since_yesterday\", 0.7, 0.05),\n",
    "    (\"location\", \"downtown_office_building\", 0.6, 0.03),\n",
    "    (\"contact_preference\", \"phone_immediate_response\", 0.8, 0.02)\n",
    "]\n",
    "\n",
    "for key, value, importance, cost in intake_data:\n",
    "    coordinator.agents[\"intake_agent\"].add(key, value, importance=importance, storage_cost=cost)\n",
    "    print(f\"   Added: {key} (importance: {importance})\")\n",
    "\n",
    "# Add critical info to shared context\n",
    "coordinator.add_to_shared(\"customer_info\", \"David Chen, Account #12789\", 0.95, 0.04)\n",
    "coordinator.add_to_shared(\"issue_type\", \"internet_connectivity_problem\", 1.0, 0.03)\n",
    "coordinator.add_to_shared(\"urgency\", \"high_business_critical\", 0.9, 0.02)\n",
    "\n",
    "# Advance time (intake agent processes quickly)\n",
    "for _ in range(3):\n",
    "    coordinator.agents[\"intake_agent\"].step()\n",
    "\n",
    "# Phase 2: Transfer to Specialist\n",
    "print(\"\\n2Ô∏è‚É£ Transfer to Specialist Agent:\")\n",
    "transfer_keys = [\"customer_info\", \"issue_type\", \"urgency\", \"customer_tier\", \"initial_symptoms\"]\n",
    "transferred = coordinator.transfer_context(\"intake_agent\", \"specialist_agent\", transfer_keys)\n",
    "\n",
    "for key, old_weight, new_importance in transferred:\n",
    "    print(f\"   Transferred: {key} (weight: {old_weight:.3f} ‚Üí importance: {new_importance:.3f})\")\n",
    "\n",
    "# Specialist adds technical context\n",
    "print(\"\\nüîß Specialist Agent Phase:\")\n",
    "specialist_data = [\n",
    "    (\"technical_details\", \"DNS_resolution_failures_detected\", 0.9, 0.06),\n",
    "    (\"troubleshooting_history\", [\"ping_test_failed\", \"traceroute_timeout\", \"DNS_flush_attempted\"], 0.8, 0.08),\n",
    "    (\"solution_attempts\", [\"router_restart\", \"DNS_server_change\"], 0.85, 0.07),\n",
    "    (\"root_cause\", \"ISP_DNS_server_maintenance\", 0.95, 0.05),\n",
    "    (\"estimated_resolution\", \"2_hours_ISP_completion\", 0.9, 0.04),\n",
    "    (\"workaround_provided\", \"temporary_public_DNS_configuration\", 0.8, 0.06)\n",
    "]\n",
    "\n",
    "for key, value, importance, cost in specialist_data:\n",
    "    coordinator.agents[\"specialist_agent\"].add(key, value, importance=importance, storage_cost=cost)\n",
    "    print(f\"   Added: {key} (importance: {importance})\")\n",
    "\n",
    "# Update shared context with resolution info\n",
    "coordinator.add_to_shared(\"root_cause\", \"ISP_DNS_server_maintenance\", 0.95, 0.05)\n",
    "coordinator.add_to_shared(\"solution_status\", \"workaround_provided_monitoring\", 0.9, 0.04)\n",
    "\n",
    "# Advance time (specialist works longer)\n",
    "for _ in range(8):\n",
    "    coordinator.agents[\"specialist_agent\"].step()\n",
    "\n",
    "# Phase 3: Transfer to Follow-up\n",
    "print(\"\\n3Ô∏è‚É£ Transfer to Follow-up Agent:\")\n",
    "followup_transfer_keys = [\"customer_info\", \"root_cause\", \"solution_attempts\", \"estimated_resolution\"]\n",
    "transferred_followup = coordinator.transfer_context(\"specialist_agent\", \"followup_agent\", followup_transfer_keys)\n",
    "\n",
    "for key, old_weight, new_importance in transferred_followup:\n",
    "    print(f\"   Transferred: {key} (weight: {old_weight:.3f} ‚Üí importance: {new_importance:.3f})\")\n",
    "\n",
    "# Follow-up agent adds relationship context\n",
    "print(\"\\nüìû Follow-up Agent Phase:\")\n",
    "followup_data = [\n",
    "    (\"resolution_status\", \"fully_resolved_customer_satisfied\", 1.0, 0.03),\n",
    "    (\"customer_satisfaction\", \"9_out_of_10_rating\", 0.9, 0.04),\n",
    "    (\"follow_up_schedule\", \"1_week_check_in_scheduled\", 0.8, 0.03),\n",
    "    (\"relationship_notes\", \"appreciates_proactive_communication\", 0.7, 0.05),\n",
    "    (\"future_opportunities\", \"interested_in_redundancy_solutions\", 0.6, 0.06)\n",
    "]\n",
    "\n",
    "for key, value, importance, cost in followup_data:\n",
    "    coordinator.agents[\"followup_agent\"].add(key, value, importance=importance, storage_cost=cost)\n",
    "    print(f\"   Added: {key} (importance: {importance})\")\n",
    "\n",
    "# Final shared context update\n",
    "coordinator.add_to_shared(\"case_resolution\", \"successful_high_satisfaction\", 1.0, 0.04)\n",
    "coordinator.add_to_shared(\"customer_satisfaction\", \"9_out_of_10_rating\", 0.9, 0.03)\n",
    "\n",
    "# Advance time for follow-up\n",
    "for _ in range(5):\n",
    "    coordinator.agents[\"followup_agent\"].step()\n",
    "    coordinator.shared_context.step()\n",
    "\n",
    "# Analyze final memory state across all agents\n",
    "print(\"\\nüìä Final Multi-Agent Memory Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "stats = coordinator.get_agent_stats()\n",
    "\n",
    "for agent_name, agent_stats in stats.items():\n",
    "    memory_stats = agent_stats[\"memory_stats\"]\n",
    "    cost_report = agent_stats[\"cost_report\"]\n",
    "    \n",
    "    print(f\"\\n{agent_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"   Active items: {agent_stats['active_items']}\")\n",
    "    print(f\"   Total items: {memory_stats['total_items']}\")\n",
    "    print(f\"   Average decay: {memory_stats['avg_decay']:.3f}\")\n",
    "    \n",
    "    if cost_report:\n",
    "        print(f\"   Total cost: ${cost_report['total_cost']:.3f}\")\n",
    "        print(f\"   Budget utilization: {cost_report['budget_utilization']:.1%}\")\n",
    "        print(f\"   Cost efficiency: {cost_report['cost_efficiency']:.1f}\")\n",
    "\n",
    "# Demonstrate memory coordination benefits\n",
    "print(\"\\nüí° Multi-Agent Memory Coordination Benefits:\")\n",
    "benefits = [\n",
    "    \"Context preservation across agent handoffs\",\n",
    "    \"Specialized memory patterns for different agent types\",\n",
    "    \"Shared knowledge base for critical information\",\n",
    "    \"Cost optimization per agent's operational needs\",\n",
    "    \"Importance adjustment based on agent priorities\"\n",
    "]\n",
    "\n",
    "for benefit in benefits:\n",
    "    print(f\"   ‚úÖ {benefit}\")\n",
    "\n",
    "# Show shared context effectiveness\n",
    "shared_active = coordinator.shared_context.get_active(threshold=0.3)\n",
    "print(f\"\\nüåê Shared Context Effectiveness:\")\n",
    "print(f\"   Shared memories retained: {len(shared_active)}\")\n",
    "print(f\"   Available to all agents for future cases\")\n",
    "for key, value, weight in shared_active:\n",
    "    print(f\"   {key}: {weight:.3f} weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Chapter 5: Performance Optimization & Scaling\n",
    "\n",
    "Learn to optimize ContextDecay for high-performance production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Chapter 5: Performance Optimization & Scaling\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "print(\"üéØ High-Performance Context Management for Production\")\n",
    "\n",
    "# Performance-optimized context manager\n",
    "class HighPerformanceContextManager:\n",
    "    def __init__(self, agent_id: str):\n",
    "        self.agent_id = agent_id\n",
    "        self.context = ContextDecay(\n",
    "            half_life_steps=12,\n",
    "            cost_optimization=True,\n",
    "            max_context_cost=2.00\n",
    "        )\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.performance_metrics = {\n",
    "            \"add_operations\": 0,\n",
    "            \"get_active_operations\": 0,\n",
    "            \"cleanup_operations\": 0,\n",
    "            \"total_add_time\": 0.0,\n",
    "            \"total_query_time\": 0.0,\n",
    "            \"total_cleanup_time\": 0.0,\n",
    "            \"memory_pressure_events\": 0\n",
    "        }\n",
    "        \n",
    "        # Configuration\n",
    "        self.config = {\n",
    "            \"cleanup_interval\": 50,      # Clean up every 50 operations\n",
    "            \"batch_size\": 20,            # Process items in batches\n",
    "            \"memory_pressure_threshold\": 1000,  # Max items before pressure\n",
    "            \"performance_warning_ms\": 100      # Warn if operations take > 100ms\n",
    "        }\n",
    "    \n",
    "    def add_context(self, key: str, value, importance: float, storage_cost: float = 0.01):\n",
    "        \"\"\"Add context with performance monitoring\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check memory pressure\n",
    "        if len(self.context._items) > self.config[\"memory_pressure_threshold\"]:\n",
    "            self.performance_metrics[\"memory_pressure_events\"] += 1\n",
    "            self._emergency_cleanup()\n",
    "        \n",
    "        # Add to context\n",
    "        self.context.add(key, value, importance=importance, storage_cost=storage_cost)\n",
    "        \n",
    "        # Track performance\n",
    "        operation_time = (time.time() - start_time) * 1000  # ms\n",
    "        self.performance_metrics[\"add_operations\"] += 1\n",
    "        self.performance_metrics[\"total_add_time\"] += operation_time\n",
    "        \n",
    "        if operation_time > self.config[\"performance_warning_ms\"]:\n",
    "            print(f\"‚ö†Ô∏è  Slow add operation: {operation_time:.1f}ms for {key}\")\n",
    "        \n",
    "        # Periodic cleanup\n",
    "        if self.performance_metrics[\"add_operations\"] % self.config[\"cleanup_interval\"] == 0:\n",
    "            self._scheduled_cleanup()\n",
    "    \n",
    "    def get_relevant_context(self, threshold: float = 0.3, max_items: int = 100):\n",
    "        \"\"\"Get relevant context with performance optimization\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get active context\n",
    "        active_context = self.context.get_active(threshold=threshold)\n",
    "        \n",
    "        # Limit results for performance\n",
    "        if len(active_context) > max_items:\n",
    "            active_context = active_context[:max_items]\n",
    "        \n",
    "        # Track performance\n",
    "        operation_time = (time.time() - start_time) * 1000\n",
    "        self.performance_metrics[\"get_active_operations\"] += 1\n",
    "        self.performance_metrics[\"total_query_time\"] += operation_time\n",
    "        \n",
    "        if operation_time > self.config[\"performance_warning_ms\"]:\n",
    "            print(f\"‚ö†Ô∏è  Slow query operation: {operation_time:.1f}ms\")\n",
    "        \n",
    "        return active_context\n",
    "    \n",
    "    def _scheduled_cleanup(self):\n",
    "        \"\"\"Scheduled cleanup to maintain performance\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        removed = self.context.clear_expired(threshold=0.1)\n",
    "        \n",
    "        operation_time = (time.time() - start_time) * 1000\n",
    "        self.performance_metrics[\"cleanup_operations\"] += 1\n",
    "        self.performance_metrics[\"total_cleanup_time\"] += operation_time\n",
    "        \n",
    "        if removed > 0:\n",
    "            print(f\"üßπ Scheduled cleanup: removed {removed} expired items ({operation_time:.1f}ms)\")\n",
    "    \n",
    "    def _emergency_cleanup(self):\n",
    "        \"\"\"Emergency cleanup under memory pressure\"\"\"\n",
    "        print(f\"üö® Emergency cleanup triggered: {len(self.context._items)} items\")\n",
    "        \n",
    "        # More aggressive cleanup\n",
    "        removed = self.context.clear_expired(threshold=0.2)\n",
    "        print(f\"   Removed {removed} items with threshold 0.2\")\n",
    "        \n",
    "        # If still too many items, remove by cost effectiveness\n",
    "        if len(self.context._items) > self.config[\"memory_pressure_threshold\"] * 0.8:\n",
    "            self.context._prune_by_cost_effectiveness()\n",
    "            print(f\"   Applied cost-based pruning\")\n",
    "    \n",
    "    def get_performance_report(self):\n",
    "        \"\"\"Get detailed performance metrics\"\"\"\n",
    "        metrics = self.performance_metrics.copy()\n",
    "        \n",
    "        # Calculate averages\n",
    "        if metrics[\"add_operations\"] > 0:\n",
    "            metrics[\"avg_add_time_ms\"] = metrics[\"total_add_time\"] / metrics[\"add_operations\"]\n",
    "        \n",
    "        if metrics[\"get_active_operations\"] > 0:\n",
    "            metrics[\"avg_query_time_ms\"] = metrics[\"total_query_time\"] / metrics[\"get_active_operations\"]\n",
    "        \n",
    "        if metrics[\"cleanup_operations\"] > 0:\n",
    "            metrics[\"avg_cleanup_time_ms\"] = metrics[\"total_cleanup_time\"] / metrics[\"cleanup_operations\"]\n",
    "        \n",
    "        # Add context stats\n",
    "        metrics[\"current_items\"] = len(self.context._items)\n",
    "        metrics[\"current_cost\"] = self.context._current_cost\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Initialize high-performance manager\n",
    "perf_manager = HighPerformanceContextManager(\"high_freq_trading_agent\")\n",
    "print(\"‚úÖ High-performance context manager initialized\")\n",
    "\n",
    "# Simulate high-frequency operations\n",
    "print(\"\\nüöÄ High-Frequency Operation Simulation:\")\n",
    "print(\"Simulating 200 rapid context additions...\")\n",
    "\n",
    "# Generate realistic high-frequency data\n",
    "operation_types = [\n",
    "    (\"market_tick\", 0.6, 0.001),      # High volume, low importance, low cost\n",
    "    (\"trade_signal\", 0.9, 0.005),     # Medium volume, high importance, low cost\n",
    "    (\"risk_update\", 0.8, 0.003),      # Medium volume, high importance, low cost\n",
    "    (\"news_event\", 0.7, 0.010),       # Low volume, medium importance, medium cost\n",
    "    (\"model_update\", 0.95, 0.020),    # Very low volume, very high importance, high cost\n",
    "]\n",
    "\n",
    "# Simulate operations\n",
    "start_simulation = time.time()\n",
    "\n",
    "for i in range(200):\n",
    "    # Choose operation type with realistic distribution\n",
    "    if i % 20 == 0:  # Model update every 20 operations\n",
    "        op_type, importance, cost = operation_types[4]\n",
    "        key = f\"{op_type}_{i}\"\n",
    "        value = f\"Model weights updated at step {i}\"\n",
    "    elif i % 10 == 0:  # News event every 10 operations\n",
    "        op_type, importance, cost = operation_types[3]\n",
    "        key = f\"{op_type}_{i}\"\n",
    "        value = f\"Market news: {random.choice(['Fed announcement', 'Earnings report', 'Economic data'])}\"\n",
    "    elif i % 3 == 0:  # Risk update every 3 operations\n",
    "        op_type, importance, cost = operation_types[2]\n",
    "        key = f\"{op_type}_{i}\"\n",
    "        value = f\"Portfolio risk: {random.uniform(0.1, 0.8):.3f}\"\n",
    "    elif i % 2 == 0:  # Trade signal every 2 operations\n",
    "        op_type, importance, cost = operation_types[1]\n",
    "        key = f\"{op_type}_{i}\"\n",
    "        value = f\"Signal: {random.choice(['BUY', 'SELL', 'HOLD'])} confidence {random.uniform(0.6, 0.95):.2f}\"\n",
    "    else:  # Market tick for the rest\n",
    "        op_type, importance, cost = operation_types[0]\n",
    "        key = f\"{op_type}_{i}\"\n",
    "        value = f\"Price: ${random.uniform(100, 200):.2f}\"\n",
    "    \n",
    "    # Add slight randomness to importance and cost\n",
    "    actual_importance = importance + random.uniform(-0.1, 0.1)\n",
    "    actual_importance = max(0.1, min(1.0, actual_importance))\n",
    "    \n",
    "    actual_cost = cost * random.uniform(0.8, 1.2)\n",
    "    \n",
    "    # Add to context\n",
    "    perf_manager.add_context(key, value, actual_importance, actual_cost)\n",
    "    \n",
    "    # Advance time occasionally\n",
    "    if i % 10 == 0:\n",
    "        perf_manager.context.step()\n",
    "    \n",
    "    # Occasional queries\n",
    "    if i % 25 == 0:\n",
    "        relevant = perf_manager.get_relevant_context(threshold=0.4, max_items=50)\n",
    "        if i % 100 == 0:  # Less frequent logging\n",
    "            print(f\"   Step {i}: {len(relevant)} relevant items retrieved\")\n",
    "\n",
    "total_simulation_time = time.time() - start_simulation\n",
    "print(f\"\\n‚è±Ô∏è  Simulation completed in {total_simulation_time:.3f} seconds\")\n",
    "\n",
    "# Performance analysis\n",
    "print(\"\\nüìä Performance Analysis:\")\n",
    "perf_report = perf_manager.get_performance_report()\n",
    "\n",
    "print(f\"   Operations completed:\")\n",
    "print(f\"     Add operations: {perf_report['add_operations']}\")\n",
    "print(f\"     Query operations: {perf_report['get_active_operations']}\")\n",
    "print(f\"     Cleanup operations: {perf_report['cleanup_operations']}\")\n",
    "\n",
    "print(f\"\\n   Average operation times:\")\n",
    "print(f\"     Add: {perf_report.get('avg_add_time_ms', 0):.2f}ms\")\n",
    "print(f\"     Query: {perf_report.get('avg_query_time_ms', 0):.2f}ms\")\n",
    "print(f\"     Cleanup: {perf_report.get('avg_cleanup_time_ms', 0):.2f}ms\")\n",
    "\n",
    "print(f\"\\n   Memory management:\")\n",
    "print(f\"     Current items: {perf_report['current_items']}\")\n",
    "print(f\"     Current cost: ${perf_report['current_cost']:.3f}\")\n",
    "print(f\"     Memory pressure events: {perf_report['memory_pressure_events']}\")\n",
    "\n",
    "print(f\"\\n   Throughput:\")\n",
    "operations_per_second = perf_report['add_operations'] / total_simulation_time\n",
    "print(f\"     Add operations/sec: {operations_per_second:.1f}\")\n",
    "\n",
    "# Stress test - burst operations\n",
    "print(\"\\nüî• Stress Test: Burst Operations\")\n",
    "print(\"Simulating 100 rapid-fire additions...\")\n",
    "\n",
    "burst_start = time.time()\n",
    "for i in range(100):\n",
    "    key = f\"burst_op_{i}\"\n",
    "    value = f\"Burst data {i}\"\n",
    "    importance = random.uniform(0.3, 0.9)\n",
    "    cost = random.uniform(0.001, 0.005)\n",
    "    \n",
    "    perf_manager.add_context(key, value, importance, cost)\n",
    "\n",
    "burst_time = time.time() - burst_start\n",
    "burst_ops_per_sec = 100 / burst_time\n",
    "\n",
    "print(f\"   Burst completed in {burst_time:.3f} seconds\")\n",
    "print(f\"   Burst throughput: {burst_ops_per_sec:.1f} ops/sec\")\n",
    "\n",
    "# Final performance assessment\n",
    "final_report = perf_manager.get_performance_report()\n",
    "final_relevant = perf_manager.get_relevant_context(threshold=0.3)\n",
    "\n",
    "print(f\"\\nüéØ Final Performance Assessment:\")\n",
    "print(f\"   Total operations: {final_report['add_operations']}\")\n",
    "print(f\"   Items retained: {len(final_relevant)}\")\n",
    "print(f\"   Retention rate: {(len(final_relevant) / final_report['add_operations']) * 100:.1f}%\")\n",
    "print(f\"   Memory efficiency: {final_report['current_cost'] / len(final_relevant):.4f} $/item\")\n",
    "\n",
    "# Production optimization recommendations\n",
    "print(\"\\nüöÄ Production Optimization Recommendations:\")\n",
    "recommendations = [\n",
    "    f\"‚úÖ Achieved {operations_per_second:.0f} ops/sec sustained throughput\",\n",
    "    f\"‚úÖ Memory pressure handled with {final_report['memory_pressure_events']} emergency cleanups\",\n",
    "    f\"‚úÖ Average add latency: {final_report.get('avg_add_time_ms', 0):.1f}ms (target: <10ms)\",\n",
    "    f\"‚úÖ Query performance: {final_report.get('avg_query_time_ms', 0):.1f}ms (target: <50ms)\",\n",
    "    f\"‚úÖ Cost efficiency: ${final_report['current_cost']:.3f} for {len(final_relevant)} items\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"   {rec}\")\n",
    "\n",
    "print(\"\\n‚ö° Performance optimization strategies validated:\")\n",
    "strategies = [\n",
    "    \"Periodic automated cleanup prevents memory bloat\",\n",
    "    \"Emergency cleanup handles unexpected memory pressure\",\n",
    "    \"Cost-based pruning maintains budget compliance\",\n",
    "    \"Performance monitoring identifies bottlenecks\",\n",
    "    \"Batch processing optimizes query operations\"\n",
    "]\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"   üìà {strategy}\")\n",
    "\n",
    "print(f\"\\nüèÜ Ready for production deployment at scale!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Chapter 6: Real-World Applications & Best Practices\n",
    "\n",
    "Explore practical applications and proven patterns for deploying ContextDecay in production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Chapter 6: Real-World Applications & Best Practices\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"üìö Comprehensive Application Examples:\")\n",
    "\n",
    "# Application 1: E-commerce Recommendation Engine\n",
    "print(\"\\nüõí Application 1: E-commerce Recommendation Engine\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "ecommerce_memory = ContextDecay(\n",
    "    half_life_steps=15,  # 15 user actions\n",
    "    cost_optimization=True,\n",
    "    max_context_cost=1.50\n",
    ")\n",
    "\n",
    "# Simulate user shopping behavior\n",
    "shopping_actions = [\n",
    "    (\"viewed_product\", \"laptop_dell_xps13\", 0.6, 0.02, \"Product view\"),\n",
    "    (\"added_to_cart\", \"laptop_dell_xps13\", 0.9, 0.05, \"Strong purchase intent\"),\n",
    "    (\"viewed_category\", \"electronics_laptops\", 0.7, 0.03, \"Category interest\"),\n",
    "    (\"search_query\", \"lightweight laptop programming\", 0.8, 0.04, \"User intent\"),\n",
    "    (\"price_filter\", \"1000_2000_range\", 0.8, 0.02, \"Budget constraint\"),\n",
    "    (\"viewed_reviews\", \"laptop_dell_xps13_reviews\", 0.7, 0.06, \"Research behavior\"),\n",
    "    (\"compared_products\", [\"laptop_dell_xps13\", \"macbook_air_m2\"], 0.85, 0.08, \"Active comparison\"),\n",
    "    (\"abandoned_cart\", \"session_timeout\", 0.3, 0.01, \"Session ended\"),\n",
    "    (\"returned_later\", \"same_day_return\", 0.9, 0.03, \"High intent\"),\n",
    "    (\"purchased\", \"laptop_dell_xps13\", 1.0, 0.10, \"Conversion!\"),\n",
    "    (\"browsed_accessories\", \"laptop_cases_mice\", 0.6, 0.04, \"Cross-sell opportunity\")\n",
    "]\n",
    "\n",
    "print(\"User shopping journey:\")\n",
    "for i, (key, value, importance, cost, description) in enumerate(shopping_actions):\n",
    "    ecommerce_memory.add(key, value, importance=importance, storage_cost=cost)\n",
    "    ecommerce_memory.step()  # Each action advances time\n",
    "    print(f\"   {i+1:2d}. {description}: {key}\")\n",
    "\n",
    "# Analyze shopping behavior retention\n",
    "shopping_context = ecommerce_memory.get_active(threshold=0.3)\n",
    "print(f\"\\nRecommendation context (threshold 0.3): {len(shopping_context)} items\")\n",
    "for key, value, weight in shopping_context:\n",
    "    print(f\"   {key}: {weight:.3f} weight\")\n",
    "\n",
    "# Application 2: Medical Diagnosis Assistant\n",
    "print(\"\\nüè• Application 2: Medical Diagnosis Assistant\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Custom decay for medical context - longer retention for critical info\n",
    "def medical_decay(importance, steps, half_life):\n",
    "    \"\"\"Medical context decay - critical symptoms persist longer\"\"\"\n",
    "    if importance > 0.9:  # Critical symptoms\n",
    "        return importance * (0.5 ** (steps / (half_life * 2)))\n",
    "    elif importance > 0.7:  # Important symptoms\n",
    "        return importance * (0.5 ** (steps / (half_life * 1.5)))\n",
    "    else:  # General information\n",
    "        return importance * (0.5 ** (steps / half_life))\n",
    "\n",
    "medical_memory = ContextDecay(\n",
    "    half_life_steps=20,  # Longer retention for medical context\n",
    "    decay_function=medical_decay,\n",
    "    cost_optimization=True,\n",
    "    max_context_cost=3.00  # Higher budget for critical application\n",
    ")\n",
    "\n",
    "# Simulate patient consultation\n",
    "consultation_data = [\n",
    "    (\"chief_complaint\", \"chest_pain_shortness_of_breath\", 1.0, 0.15, \"Primary concern\"),\n",
    "    (\"symptom_duration\", \"3_days_worsening\", 0.95, 0.08, \"Timeline critical\"),\n",
    "    (\"pain_severity\", \"7_out_of_10\", 0.9, 0.05, \"Pain assessment\"),\n",
    "    (\"medical_history\", \"hypertension_diabetes\", 0.9, 0.12, \"Risk factors\"),\n",
    "    (\"current_medications\", [\"metformin\", \"lisinopril\"], 0.85, 0.10, \"Drug interactions\"),\n",
    "    (\"vital_signs\", \"BP_150_95_HR_105_irregular\", 0.95, 0.08, \"Objective data\"),\n",
    "    (\"family_history\", \"father_heart_attack_age_55\", 0.8, 0.07, \"Genetic risk\"),\n",
    "    (\"lifestyle_factors\", \"smoker_sedentary\", 0.7, 0.06, \"Risk modifiers\"),\n",
    "    (\"previous_tests\", \"EKG_6_months_ago_normal\", 0.6, 0.09, \"Historical data\"),\n",
    "    (\"physical_exam\", \"heart_murmur_detected\", 0.9, 0.11, \"Clinical findings\"),\n",
    "    (\"differential_diagnosis\", [\"MI\", \"angina\", \"arrhythmia\"], 0.95, 0.15, \"Working diagnosis\"),\n",
    "    (\"recommended_tests\", [\"EKG\", \"troponin\", \"echo\"], 0.9, 0.12, \"Next steps\")\n",
    "]\n",
    "\n",
    "print(\"Patient consultation progression:\")\n",
    "for key, value, importance, cost, description in consultation_data:\n",
    "    medical_memory.add(key, value, importance=importance, storage_cost=cost)\n",
    "    print(f\"   {description}: importance {importance}\")\n",
    "\n",
    "# Simulate time progression (several follow-up visits)\n",
    "for _ in range(10):\n",
    "    medical_memory.step()\n",
    "\n",
    "# Analyze medical context retention\n",
    "medical_context = medical_memory.get_active(threshold=0.4)\n",
    "print(f\"\\nCritical medical context retained: {len(medical_context)} items\")\n",
    "critical_items = [item for item in medical_context if item[2] > 0.7]\n",
    "print(f\"High-priority items (weight > 0.7): {len(critical_items)}\")\n",
    "\n",
    "# Application 3: Financial Trading Bot\n",
    "print(\"\\nüí∞ Application 3: Financial Trading Bot\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Multi-timeframe memory for different types of financial data\n",
    "trading_memory = {\n",
    "    \"tick_data\": ContextDecay(half_life_steps=3),      # Very short-term\n",
    "    \"technical_signals\": ContextDecay(half_life_steps=10),  # Short-term\n",
    "    \"fundamental_data\": ContextDecay(half_life_steps=50),   # Long-term\n",
    "    \"risk_metrics\": ContextDecay(half_life_steps=20)       # Medium-term\n",
    "}\n",
    "\n",
    "# Simulate trading day\n",
    "trading_events = [\n",
    "    (\"tick_data\", \"price_spike\", \"AAPL_sudden_jump_2%\", 0.8),\n",
    "    (\"technical_signals\", \"rsi_overbought\", \"RSI_above_70\", 0.9),\n",
    "    (\"fundamental_data\", \"earnings_beat\", \"Q3_earnings_15%_above_estimate\", 0.95),\n",
    "    (\"risk_metrics\", \"portfolio_var\", \"VaR_increased_to_2.5%\", 0.85),\n",
    "    (\"tick_data\", \"volume_surge\", \"trading_volume_3x_average\", 0.7),\n",
    "    (\"technical_signals\", \"breakout_confirmed\", \"resistance_level_broken\", 0.9),\n",
    "    (\"fundamental_data\", \"analyst_upgrade\", \"Goldman_raises_target_to_180\", 0.8),\n",
    "    (\"risk_metrics\", \"correlation_change\", \"correlation_with_SPY_decreased\", 0.6)\n",
    "]\n",
    "\n",
    "print(\"Trading day events:\")\n",
    "for category, key, value, importance in trading_events:\n",
    "    trading_memory[category].add(key, value, importance=importance)\n",
    "    print(f\"   {category}: {key} (importance: {importance})\")\n",
    "\n",
    "# Advance time for different categories\n",
    "for _ in range(15):\n",
    "    for memory in trading_memory.values():\n",
    "        memory.step()\n",
    "\n",
    "# Analyze what persists in each timeframe\n",
    "print(\"\\nMemory persistence by timeframe:\")\n",
    "for category, memory in trading_memory.items():\n",
    "    active = memory.get_active(threshold=0.3)\n",
    "    print(f\"   {category}: {len(active)} items still relevant\")\n",
    "\n",
    "# Best Practices Summary\n",
    "print(\"\\nüéØ Best Practices Summary:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "best_practices = {\n",
    "    \"üîß Configuration\": [\n",
    "        \"Match half-life to domain-specific information lifecycle\",\n",
    "        \"Set importance scores based on business impact\",\n",
    "        \"Configure cost budgets aligned with operational constraints\",\n",
    "        \"Choose decay functions that match information aging patterns\"\n",
    "    ],\n",
    "    \n",
    "    \"‚ö° Performance\": [\n",
    "        \"Monitor memory usage and implement automatic cleanup\",\n",
    "        \"Use cost optimization to balance quality and efficiency\",\n",
    "        \"Batch operations when possible for better throughput\",\n",
    "        \"Set up alerts for memory pressure and performance degradation\"\n",
    "    ],\n",
    "    \n",
    "    \"üè≠ Production\": [\n",
    "        \"Implement gradual rollout with A/B testing\",\n",
    "        \"Monitor business metrics impact, not just technical metrics\",\n",
    "        \"Have fallback strategies for memory system failures\",\n",
    "        \"Regular audits of memory patterns and cost efficiency\"\n",
    "    ],\n",
    "    \n",
    "    \"üîí Security & Compliance\": [\n",
    "        \"Implement data retention policies for sensitive information\",\n",
    "        \"Ensure memory cleanup meets regulatory requirements\",\n",
    "        \"Audit trails for memory management decisions\",\n",
    "        \"Privacy-compliant handling of personal data in memory\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"   ‚Ä¢ {practice}\")\n",
    "\n",
    "# Common Pitfalls and Solutions\n",
    "print(\"\\n‚ùå Common Pitfalls and Solutions:\")\n",
    "pitfalls = {\n",
    "    \"Over-retention\": \"Setting half-life too long ‚Üí Monitor memory growth and cost impact\",\n",
    "    \"Under-retention\": \"Losing critical context ‚Üí Use importance weighting and custom decay\",\n",
    "    \"Cost explosion\": \"No budget limits ‚Üí Implement cost optimization and monitoring\",\n",
    "    \"Performance degradation\": \"Large memory footprint ‚Üí Regular cleanup and memory pressure handling\",\n",
    "    \"Poor decay matching\": \"Wrong decay pattern ‚Üí Study domain patterns and test different functions\"\n",
    "}\n",
    "\n",
    "for pitfall, solution in pitfalls.items():\n",
    "    print(f\"   ‚ö†Ô∏è  {pitfall}: {solution}\")\n",
    "\n",
    "# Integration patterns\n",
    "print(\"\\nüîó Integration Patterns:\")\n",
    "patterns = {\n",
    "    \"Microservices\": \"Deploy memory management as separate service with API\",\n",
    "    \"Event-driven\": \"Trigger memory updates based on system events\",\n",
    "    \"Batch processing\": \"Periodic memory analysis and optimization jobs\",\n",
    "    \"Real-time\": \"Inline memory management with primary application logic\",\n",
    "    \"Hybrid\": \"Combine real-time updates with batch optimization\"\n",
    "}\n",
    "\n",
    "for pattern, description in patterns.items():\n",
    "    print(f\"   üèóÔ∏è  {pattern}: {description}\")\n",
    "\n",
    "print(\"\\nüèÜ ContextDecay Mastery Achieved!\")\n",
    "print(\"\\nYou're now equipped to:\")\n",
    "mastery_skills = [\n",
    "    \"Design memory systems that balance retention and efficiency\",\n",
    "    \"Implement cost-optimized context management\",\n",
    "    \"Deploy custom decay functions for domain-specific needs\",\n",
    "    \"Scale memory management for high-performance applications\",\n",
    "    \"Coordinate memory across multi-agent systems\",\n",
    "    \"Monitor and optimize memory systems in production\"\n",
    "]\n",
    "\n",
    "for skill in mastery_skills:\n",
    "    print(f\"   üéØ {skill}\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to build intelligent memory systems that scale!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary & Next Steps\n",
    "\n",
    "Congratulations! You've mastered the art of agent memory management with ContextDecay. Here's what you've accomplished:\n",
    "\n",
    "### üß† Core Concepts Mastered\n",
    "- **Temporal Decay Models**: Understanding how memories naturally fade and implementing appropriate decay functions\n",
    "- **Importance-Based Retention**: Keeping critical information longer while efficiently managing less important data\n",
    "- **Cost Optimization**: Balancing memory richness with budget constraints through intelligent pruning\n",
    "- **Performance Scaling**: Optimizing memory systems for high-throughput production environments\n",
    "- **Multi-Agent Coordination**: Managing shared and specialized memory contexts across agent systems\n",
    "\n",
    "### üõ†Ô∏è Practical Skills Developed\n",
    "- Custom decay function implementation for domain-specific needs\n",
    "- Cost-conscious memory management with automatic budget compliance\n",
    "- High-performance memory systems with automatic cleanup and monitoring\n",
    "- Multi-timeframe memory coordination for complex applications\n",
    "- Production deployment patterns with monitoring and optimization\n",
    "\n",
    "### üåü Real-World Applications\n",
    "- **Conversational AI**: Natural memory fade for chat interactions\n",
    "- **E-commerce**: User behavior tracking with relevance decay\n",
    "- **Medical Systems**: Critical symptom persistence with safety requirements\n",
    "- **Financial Trading**: Multi-timeframe data management for decision making\n",
    "- **Customer Service**: Context transfer between specialized agents\n",
    "\n",
    "### üìö Continue Your Journey\n",
    "1. **06_cost_alerts.ipynb** - Learn advanced cost monitoring and alerting systems\n",
    "2. **01_argentum_overview.ipynb** - Review the complete platform integration\n",
    "3. **Production Implementation** - Apply these concepts to your agent systems\n",
    "\n",
    "### ü§ù Community & Resources\n",
    "- **GitHub**: [https://github.com/MarsZDF/argentum](https://github.com/MarsZDF/argentum)\n",
    "- **Documentation**: [https://argentum-agent.readthedocs.io](https://argentum-agent.readthedocs.io)\n",
    "- **Community**: Join discussions and share your implementations\n",
    "\n",
    "---\n",
    "\n",
    "*You now possess the knowledge to build memory systems that are not just intelligent, but also efficient, cost-effective, and production-ready! üß†‚ú®*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": [],
   "name": "03_context_decay.ipynb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}